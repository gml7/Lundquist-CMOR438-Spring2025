{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "# Load the iris dataset from seaborn\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "\n",
    "# Filter the dataset to only include 'versicolor' and 'setosa' species\n",
    "filtered_iris = iris[iris['species'].isin(['versicolor', 'setosa'])]\n",
    "\n",
    "# Select only 'sepal_length' and 'sepal_width' variables\n",
    "filtered_iris = filtered_iris[['sepal_length', 'sepal_width', 'species']]\n",
    "\n",
    "# Label 'setosa' as 1 and 'versicolor' as -1\n",
    "filtered_iris['species'] = filtered_iris['species'].map({'setosa': 1, 'versicolor': -1})\n",
    "\n",
    "sepal_size_inputs = filtered_iris[['sepal_length', 'sepal_width']].to_numpy()\n",
    "target_species_output = filtered_iris['species'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "#234567891123456789212345678931234567894123456789512345678961234567897123456789\n",
    "#                                python docstring limit: 72 characters |      \n",
    "#                                            python code limit: 79 characters |\n",
    "# test change\n",
    "class SingleNeuron(object):\n",
    "    \"\"\"\n",
    "    A class used to represent a single neuron (in the machine learning \n",
    "    sense).\n",
    "\n",
    "    Design notes: the model type and the activation function are \n",
    "    intended to be immutable, though of course python still lets you mutate them.\n",
    "    \"\"\"\n",
    "    type_perceptron = \"perceptron\"\n",
    "    type_linear_regression_1D = \"linear regression 1D\"\n",
    "\n",
    "    def sign(input_value):\n",
    "        \"\"\" \n",
    "\n",
    "        \"\"\"\n",
    "        if input_value >= 0:\n",
    "            return 1\n",
    "        else: \n",
    "            return 0\n",
    "        \n",
    "    def linear_1D(input_value):\n",
    "        \"\"\" \n",
    "\n",
    "        \"\"\"\n",
    "        return input_value\n",
    "    \n",
    "    # default_activation_function = sign\n",
    "\n",
    "    # def test_for_matching_ndarrays(var_1, var_2):\n",
    "    #     return type(var_1) != np.ndarray or type(var_2) != np.ndarray \\\n",
    "    #            or var_1.shape != var_1.shape\n",
    "\n",
    "    def perceptron_loss_function(predicted_outputs, \n",
    "                                 target_outputs):\n",
    "        \"\"\" \n",
    "        \n",
    "        \"\"\"\n",
    "        # if SingleNeuron.test_for_matching_ndarrays(predicted_values, true_values) == False:\n",
    "        #     raise TypeError\n",
    "        return (1/4) * np.sum((predicted_outputs - target_outputs)**2)\n",
    "    \n",
    "    def perceptron_stochastic_gradient(predicted_output, \n",
    "                                       target_output):\n",
    "        \"\"\" \n",
    "        \n",
    "        \"\"\"\n",
    "        return (1/2) * (predicted_output - target_output)\n",
    "\n",
    "    def linear_regression_loss_function(predicted_outputs, \n",
    "                                        target_outputs):\n",
    "        \"\"\" \n",
    "        \n",
    "        \"\"\"\n",
    "        # if SingleNeuron.test_for_matching_ndarrays(predicted_values, true_values) == False:\n",
    "        #     raise TypeError\n",
    "        return (1/(2*target_outputs.size)) * np.sum((predicted_outputs - target_outputs)**2)\n",
    "        \n",
    "    def linear_regression_1D_stochastic_gradient(predicted_output, \n",
    "                                                 target_output, \n",
    "                                                 training_data_length):\n",
    "        \"\"\" \n",
    "        \n",
    "        \"\"\"\n",
    "        return (1/training_data_length) * (predicted_output - target_output)\n",
    "\n",
    "    def preactivation(input, weights, bias):\n",
    "        \"\"\" \n",
    "        \n",
    "        \"\"\"\n",
    "        assert np.shape(input) is np.shape(weights), \\\n",
    "                \"Input vector must have the same shape as weights vector.\" \\\n",
    "                + f\"{np.shape(input) = },  {np.shape(weights) = }\"\n",
    "        return np.dot(input, weights) + bias\n",
    "    \n",
    "    def __init__(self, \n",
    "                 data_dimension, \n",
    "                 model_type, \n",
    "                 weights=None, \n",
    "                 bias=None, \n",
    "                 activation_function=None):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        self.data_dimension = data_dimension \n",
    "        # A single number for each feature vector has dimension 1, \n",
    "        # a 2D vector has dimension 2, etc.\n",
    "\n",
    "        self.model_type = model_type\n",
    "\n",
    "        if activation_function is None:\n",
    "            if self.model_type == SingleNeuron.type_perceptron:\n",
    "                self.activation_function = SingleNeuron.sign\n",
    "            elif self.model_type == SingleNeuron.type_linear_regression_1D:\n",
    "                self.activation_function = SingleNeuron.linear_1D\n",
    "        else:\n",
    "            self.activation_function = activation_function\n",
    "        \n",
    "        if weights is None:\n",
    "            self.weights = np.random.randn(data_dimension)\n",
    "        else:\n",
    "            self.weights = weights\n",
    "\n",
    "        if bias is None:\n",
    "            self.bias = np.random.randn()\n",
    "        else:\n",
    "            self.bias = bias\n",
    "\n",
    "    def predict_outputs(self, \n",
    "                        inputs, \n",
    "                        weights=None, \n",
    "                        bias=None, \n",
    "                        use_current_weights_and_bias=True):\n",
    "        \"\"\" \n",
    "        \n",
    "        \"\"\"\n",
    "        if use_current_weights_and_bias:\n",
    "            weights = self.weights\n",
    "            bias = self.bias\n",
    "\n",
    "        assert (np.isscalar(inputs) and self.data_dimension == 1) \\\n",
    "                or ((not np.isscalar(inputs)) and inputs.shape[-1] == self.data_dimension), \\\n",
    "                    \"Prediction: mismatch between expected feature vector \" \\\n",
    "                        + f\"\\ndimension ({self.data_dimension = }) and input \" \\\n",
    "                        + f\"shape ({np.shape(inputs) = }).\"\n",
    "        \n",
    "        if ((not np.isscalar(inputs)) and len(inputs.shape) == 1):\n",
    "            return self.activation_function(SingleNeuron.preactivation(inputs, weights, bias))\n",
    "        else:\n",
    "            return [self.activation_function(SingleNeuron.preactivation(input, weights, bias)) for input in inputs]\n",
    "    \n",
    "    def current_weights(self):\n",
    "        \"\"\" \n",
    "        \n",
    "        \"\"\"\n",
    "        return self.weights.copy()\n",
    "    \n",
    "    def current_bias(self):\n",
    "        \"\"\" \n",
    "        \n",
    "        \"\"\"\n",
    "        return self.bias\n",
    "\n",
    "    def current_weights_and_bias(self):\n",
    "        \"\"\" \n",
    "        \n",
    "        \"\"\"\n",
    "        return (self.weights.copy(), self.bias)\n",
    "    \n",
    "    def perceptron_stochastic_gradient_update(self, \n",
    "                                              input, \n",
    "                                              target_output, \n",
    "                                              learning_rate=None):\n",
    "        \"\"\" \n",
    "        learning_rate does nothing and is just for making it more uniform to pass around update functions\n",
    "        \"\"\"\n",
    "        gradient = SingleNeuron.perceptron_stochastic_gradient(self.predict_outputs(input), target_output)\n",
    "        self.weights -= gradient * input\n",
    "        self.bias -= gradient\n",
    "        return gradient\n",
    "\n",
    "    def linear_regression_1D_stochastic_gradient_update(self, \n",
    "                                                        input, \n",
    "                                                        target_output, \n",
    "                                                        learning_rate):\n",
    "        gradient = SingleNeuron.linear_regression_1D_stochastic_gradient(self.predict_outputs(input), target_output)\n",
    "        self.weights -= learning_rate * gradient * input\n",
    "        self.bias -= learning_rate * gradient\n",
    "        return gradient\n",
    "        \n",
    "    def train(self, \n",
    "              inputs, \n",
    "              target_outputs, \n",
    "              learning_rate=0.5, \n",
    "              num_epochs=50):\n",
    "        \"\"\" \n",
    "        \n",
    "        \"\"\"\n",
    "        # if self.model_type != SingleNeuron.type_perceptron and (learning_rate == None or epochs == None):\n",
    "        #     raise ValueError(\"learning_rate and epochs must be specified for non-perceptron models\")\n",
    "        weight_bias_update = None\n",
    "        loss_function = None\n",
    "        if self.model_type is SingleNeuron.type_perceptron:\n",
    "            weight_bias_update = self.perceptron_stochastic_gradient_update\n",
    "            loss_function = SingleNeuron.perceptron_loss_function\n",
    "        elif self.model_type is SingleNeuron.type_linear_regression_1D:\n",
    "            weight_bias_update = self.linear_regression_1D_stochastic_gradient_update\n",
    "            loss_function = SingleNeuron.linear_regression_loss_function\n",
    "\n",
    "        loss_at_epoch = [-1] * (num_epochs + 1)\n",
    "        loss_at_epoch[0] = loss_function(self.predict_outputs(inputs), target_outputs)\n",
    "\n",
    "        for epoch_index in range(num_epochs):\n",
    "            for input, target_output in zip(inputs, target_outputs):\n",
    "                weight_bias_update(input, target_output, learning_rate)\n",
    "            loss_at_epoch[epoch_index+1] = loss_function(self.predict_outputs(inputs), target_outputs)\n",
    "\n",
    "# todo\n",
    "    def __repr__(self):\n",
    "        \"\"\" \n",
    "        \n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "# might not be necessary\n",
    "    # def __call__(self):\n",
    "    #     return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_test = SingleNeuron(sepal_size_inputs.shape[1], \"perceptron\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(np.shape(np.transpose(sepal_size_inputs[1]))) = 1\n",
      "np.shape(np.transpose(sepal_size_inputs)) = (2, 100)\n",
      "len(sepal_size_inputs) = 100\n",
      "type(sepal_size_inputs) = <class 'numpy.ndarray'>,  type(5) = <class 'int'>,  type(5.1) = <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(np.shape(np.transpose(sepal_size_inputs[1]))) = }\")\n",
    "print(f\"{np.shape(np.transpose(sepal_size_inputs)) = }\")\n",
    "print(f\"{len(sepal_size_inputs) = }\")\n",
    "print(f\"{type(sepal_size_inputs) = },  {type(5) = },  {type(5.1) = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuron_test.current_weights_and_bias() = (array([-1.04808694,  2.89509232]), -2.0549427790680297)\n",
      "sepal_size_inputs[1] = array([4.9, 3. ])\n",
      "SingleNeuron.preactivation(sepal_size_inputs[1], current_weights, current_bias) = np.float64(1.494708141533946)\n",
      "neuron_test.activation_function(preactivation_value) = 1\n",
      "neuron_test.predict_outputs(sepal_size_inputs[1]) = 1\n",
      "activation_values = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1]\n",
      "neuron_test.predict_outputs(inputs) = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(f\"{neuron_test.current_weights_and_bias() = }\")\n",
    "[current_weights, current_bias] = neuron_test.current_weights_and_bias()\n",
    "print(f\"{sepal_size_inputs[1] = }\")\n",
    "print(f\"{SingleNeuron.preactivation(sepal_size_inputs[1], current_weights, current_bias) = }\")\n",
    "preactivation_value = SingleNeuron.preactivation(sepal_size_inputs[1], current_weights, current_bias)\n",
    "print(f\"{neuron_test.activation_function(preactivation_value) = }\")\n",
    "print(f\"{neuron_test.predict_outputs(sepal_size_inputs[1]) = }\")\n",
    "inputs = sepal_size_inputs\n",
    "activation_values = [neuron_test.activation_function(SingleNeuron.preactivation(input, current_weights, current_bias)) for input in inputs]\n",
    "print(f\"{activation_values = }\")\n",
    "print(f\"{neuron_test.predict_outputs(inputs) = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(SingleNeuron.sign(SingleNeuron.preactivation(input, neuron_test.current_weights(), neuron_test.current_bias()))) for input in sepal_size_inputs[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SingleNeuron.sign(np.float64(3.3034947687319463))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{SingleNeuron.sign(0) = }\")\n",
    "print(f\"{SingleNeuron.linear_1D(8.3) = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{sepal_size_inputs.shape = }\")\n",
    "print(f\"{sepal_size_inputs.shape[1] = }\")\n",
    "print(f\"{sepal_size_inputs[1] = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(3)\n",
    "type(x) == np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.random.randn(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xval in x:\n",
    "    print(xval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CMOR438_env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
