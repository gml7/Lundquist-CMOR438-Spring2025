{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "# Load the iris dataset from seaborn\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "\n",
    "# Filter the dataset to only include 'versicolor' and 'setosa' species\n",
    "filtered_iris = iris[iris['species'].isin(['versicolor', 'setosa'])]\n",
    "\n",
    "# Select only 'sepal_length' and 'sepal_width' variables\n",
    "filtered_iris = filtered_iris[['sepal_length', 'sepal_width', 'species']]\n",
    "\n",
    "# Label 'setosa' as 1 and 'versicolor' as -1\n",
    "filtered_iris['species'] = filtered_iris['species'].map({'setosa': 1, 'versicolor': -1})\n",
    "\n",
    "sepal_size_inputs = filtered_iris[['sepal_length', 'sepal_width']].to_numpy()\n",
    "target_species_output = filtered_iris['species'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SingleNeuronClass import SingleNeuron\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, None, 6]\n",
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "b = [4,None,6]\n",
    "a_prev = a.copy()\n",
    "a.extend(b)\n",
    "print(a)\n",
    "print(a_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from progress.bar import ShadyBar\n",
    "#234567891123456789212345678931234567894123456789512345678961234567897123456789\n",
    "#                                python docstring limit: 72 characters |      \n",
    "#                                            python code limit: 79 characters |\n",
    "# test change\n",
    "class SingleNeuron(object):\n",
    "    \"\"\"\n",
    "    A class implementing single-neuron machine learning algorithms.\n",
    "    Implements the perceptron, linear regression classification, \n",
    "    and logistic regression classification.\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    type_perceptron [class attribute] : string\n",
    "        Contains the keyword used to indicate that the algorithm\n",
    "        used by the instance is perceptron.\n",
    "\n",
    "    type_linear_regression_1D [class attribute] : string\n",
    "        Contains the keyword used to indicate that the algorithm\n",
    "        used by the instance is one-dimensional linear regression.\n",
    "    \n",
    "                                                                       |\n",
    "\n",
    "    Methods \n",
    "    -------\n",
    "    sign(cls, input_value) [class method]\n",
    "        Used as an activation function.\n",
    "\n",
    "    linear_1D(cls, input_value) [class method]\n",
    "        Returns 'input_value'. Used as an activation function.\n",
    "\n",
    "    perceptron_loss_function(cls, predicted_outputs, target_outputs)\n",
    "            [class method]\n",
    "        The loss function for the perceptron algorithm.\n",
    "\n",
    "    perceptron_stochastic_gradient(cls, predicted_output, target_output)\n",
    "            [class method]\n",
    "        The gradient of the perceptron's loss function when it employs\n",
    "        stochastic descent.\n",
    "\n",
    "    linear_regression_loss_function(cls, predicted_outputs, \n",
    "            target_outputs) [class method]\n",
    "        The loss function for the linear regression algorithm.\n",
    "\n",
    "    linear_regression_1D_stochastic_gradient(cls, predicted_output, \n",
    "            target_output) [class method]\n",
    "        The gradient of the one-dimensional linear regression loss \n",
    "        function when it employs stochastic descent.\n",
    "\n",
    "    preactivation(cls, input, weights, bias) [class method]\n",
    "        The preactivation function for the single neuron (dot product of\n",
    "        weights and input vector, plus bias)\n",
    "\n",
    "    __init__(self, data_dimension, model_type, weights=None, bias=None, \n",
    "            activation_function=None)\n",
    "        Initializes a model for a given dimensionality of training data\n",
    "        and model type. Can \"warm start\" with a user-provided weight and\n",
    "        bias, otherwise randomizes them. Can also provide an activation\n",
    "        function if the user wants one different from the default for \n",
    "        the model type.\n",
    "\n",
    "    predict_outputs(self, inputs, weights=None, bias=None, \n",
    "            use_current_weights_and_bias=True)\n",
    "        Outputs the model's predictions given an array of feature \n",
    "        vectors. \n",
    "\n",
    "    current_weights_and_bias(self)\n",
    "        Utility function for wrapping weights and bias in a single tuple.\n",
    "\n",
    "    perceptron_stochastic_gradient_update(self, input, target_output)\n",
    "        Updates the weights of the perceptron model using stochastic\n",
    "        descent.\n",
    "    \n",
    "    linear_regression_1D_stochastic_gradient_update(self, input, \n",
    "            target_output, learning_rate, training_data_length)\n",
    "        Updates the weights of the linear regression model using \n",
    "        stochastic descent.\n",
    "    \n",
    "    train(self, inputs, target_outputs, learning_rate=0.5, \n",
    "            num_epochs=50)\n",
    "        Trains the model (updates weights) using a batch of inputs and \n",
    "        target outputs.\n",
    "\n",
    "    reset_model(self)\n",
    "        Randomizes the weights and bias of the model.\n",
    "    \n",
    "    forget_previous_training(self)\n",
    "        Resets the weights and bias to their values before the most \n",
    "        recent training.\n",
    "                                                                       |\n",
    "    \"\"\"\n",
    "    type_perceptron = \"perceptron\"\n",
    "    type_linear_regression_1D = \"linear regression 1D\"\n",
    "\n",
    "    @classmethod\n",
    "    def sign(cls, input_value):\n",
    "        \"\"\" \n",
    "        Returns the sign of input_value (+1 if it's positive, -1 if\n",
    "        it's negative). \n",
    "        \"\"\"\n",
    "        if input_value >= 0:\n",
    "            return 1\n",
    "        else: \n",
    "            return -1\n",
    "        \n",
    "    @classmethod\n",
    "    def linear_1D(cls, input_value):\n",
    "        \"\"\" \n",
    "        Returns input_value. \n",
    "        \"\"\"\n",
    "        return input_value\n",
    "\n",
    "    @classmethod\n",
    "    def perceptron_loss_function(cls, \n",
    "                                 predicted_outputs, \n",
    "                                 target_outputs):\n",
    "        \"\"\" \n",
    "        The perceptron loss function given the perceptron's predictions\n",
    "        and the target outputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        predicted_outputs : array_like\n",
    "            Perceptron's predictions. \n",
    "\n",
    "        target_outputs : array_like\n",
    "            Targets we`re training the perceptron to meet.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : float\n",
    "            The error of the prediction.\n",
    "        \"\"\"\n",
    "        return (1/4) * np.sum((predicted_outputs - target_outputs)**2)\n",
    "    \n",
    "    @classmethod\n",
    "    def perceptron_stochastic_gradient(cls, \n",
    "                                       predicted_output, \n",
    "                                       target_output):\n",
    "        \"\"\" \n",
    "        The gradient of the perceptron's loss function when it employs\n",
    "        stochastic descent.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        predicted_output : array_like\n",
    "            Perceptron's predictions. \n",
    "\n",
    "        target_output : array_like\n",
    "            Targets we`re training the perceptron to meet.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient : float\n",
    "            The gradient of the loss function at the weights used to \n",
    "            calculate predicted_output\n",
    "        \"\"\"\n",
    "        return (1/2) * (predicted_output - target_output)\n",
    "\n",
    "    @classmethod\n",
    "    def linear_regression_loss_function(cls, \n",
    "                                        predicted_outputs, \n",
    "                                        target_outputs):\n",
    "        \"\"\" \n",
    "        The loss function for the linear regression algorithm.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        predicted_outputs : array_like\n",
    "            Perceptron's predictions. \n",
    "\n",
    "        target_outputs : array_like\n",
    "            Targets we`re training the perceptron to meet.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : float\n",
    "            The error of the prediction.\n",
    "        \"\"\"\n",
    "        return (1/(2*target_outputs.size)) \\\n",
    "                * np.sum((predicted_outputs - target_outputs)**2)\n",
    "        \n",
    "    @classmethod\n",
    "    def linear_regression_1D_stochastic_gradient(cls, \n",
    "                                                 predicted_output, \n",
    "                                                 target_output, \n",
    "                                                 training_data_length):\n",
    "        \"\"\" \n",
    "        The gradient of the one-dimensional linear regression loss \n",
    "        function when it employs stochastic descent.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        predicted_output : array_like\n",
    "            The model's predictions. \n",
    "\n",
    "        target_output : array_like\n",
    "            Targets we`re training the model to meet.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient : float\n",
    "            The gradient of the loss function at the weights used to \n",
    "            calculate predicted_output\n",
    "\n",
    "        \"\"\"\n",
    "        return (1/training_data_length) * (predicted_output - target_output)\n",
    "\n",
    "    @classmethod\n",
    "    def preactivation(cls, input, weights, bias):\n",
    "        \"\"\" \n",
    "        The preactivation function for the single neuron (dot product of\n",
    "        weights and input vector, plus bias)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input : array_like\n",
    "            The vector input to the neuron.\n",
    "        \n",
    "        weights : array_like\n",
    "            The vector of weights of the model.\n",
    "        \n",
    "        bias : float\n",
    "            The bias of the model.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        preactivation_value\n",
    "            The dot product of input and weights, plus the bias.\n",
    "        \"\"\"\n",
    "        if not (np.shape(input) == np.shape(weights)):\n",
    "            raise ValueError(\"Input vector must have the same shape as weights\"\n",
    "                + \"vector.\" + f\"{np.shape(input) = },  {np.shape(weights) = }\")\n",
    "        \n",
    "        return np.dot(input, weights) + bias\n",
    "    \n",
    "    def __init__(self, \n",
    "                 data_dimension, \n",
    "                 model_type, \n",
    "                 weights=None, \n",
    "                 bias=None, \n",
    "                 activation_function=None):\n",
    "        \"\"\"\n",
    "        Initializes a model for a given dimensionality of training data\n",
    "        and model type.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_dimension : int\n",
    "            The dimensionality of the feature vectors. Shouldn't be modified\n",
    "            after initialization.\n",
    "\n",
    "        model_type : str\n",
    "            The category of model. Used to select the activation function and \n",
    "            loss function. Shouldn't be modified post-initialization.\n",
    "\n",
    "        weights : array_like\n",
    "            Optional; \"warm start\" weights can be provided by the user, \n",
    "            otherwise weights are generated from a normal distribution.\n",
    "\n",
    "        bias : array_like\n",
    "            Optional; \"warm start\" bias can be provided by the user, \n",
    "            otherwise bias is generated from a normal distribution.\n",
    "        \n",
    "        activation_function : function\n",
    "            Optional; an arbitrary activation function can be provided by the \n",
    "            user, otherwise the standard function for the model type is \n",
    "            selected.\n",
    "        \"\"\"\n",
    "        if data_dimension < 1:\n",
    "            raise ValueError(f\"Provided data dimension is {data_dimension} \" \n",
    "                             + \"but it must be a positive integer.\")\n",
    "        self.data_dimension = data_dimension \n",
    "        # A single number for each feature vector has dimension 1, \n",
    "        # a 2D vector has dimension 2, etc.\n",
    "\n",
    "        self.model_type = model_type\n",
    "\n",
    "        if activation_function is None:\n",
    "            if self.model_type == SingleNeuron.type_perceptron:\n",
    "                self.activation_function = SingleNeuron.sign\n",
    "            elif self.model_type == SingleNeuron.type_linear_regression_1D:\n",
    "                self.activation_function = SingleNeuron.linear_1D\n",
    "        else:\n",
    "            self.activation_function = activation_function\n",
    "        \n",
    "        if weights is None:\n",
    "            self.weights = np.random.randn(data_dimension)\n",
    "            if data_dimension == 1:\n",
    "                # Unwrap weights to a scalar if there's only one weight\n",
    "                # This avoids additional checks in preactivation\n",
    "                self.weights = self.weights[0]\n",
    "        else:\n",
    "            self.weights = weights\n",
    "        self.previous_weights = self.weights\n",
    "\n",
    "        if bias is None:\n",
    "            self.bias = np.random.randn()\n",
    "        else:\n",
    "            self.bias = bias\n",
    "        self.previous_bias = self.bias\n",
    "\n",
    "    def predict_outputs(self, \n",
    "                        inputs, \n",
    "                        use_current_weights_and_bias=True,\n",
    "                        weights=None, \n",
    "                        bias=None):\n",
    "        \"\"\" \n",
    "        Outputs the model's predictions given an array of feature \n",
    "        vectors. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : array_like\n",
    "            An array of feature vectors for the model to predict the outputs\n",
    "            of. Array dimensions incompatible with the dimensionality of the \n",
    "            model will raise a ValueError. Compatible array shapes are:\n",
    "                - Equal shape to that of the weights vector (single input)\n",
    "                - Final dimension equal to that of the weights vector (multiple\n",
    "                inputs)\n",
    "\n",
    "        use_current_weights_and_bias : boolean\n",
    "            Optional; if desired, the user can set this to false and provide \n",
    "            their own weights and bias whenever they require a prediction. By \n",
    "            default set to true, so this function uses the model's current \n",
    "            weights and bias.\n",
    "\n",
    "        weights : array_like\n",
    "            Optional; weight vector to use if the user desires to calculate \n",
    "            the output with weights other than the model's current weights.\n",
    "        \n",
    "        bias : float\n",
    "            Optional; bias to use if the user desires to calculate \n",
    "            the output with bias other than the model's current bias.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        outputs : array_like\n",
    "            The model's outputs corresponding to each input.\n",
    "        \"\"\"\n",
    "        if use_current_weights_and_bias:\n",
    "            weights = self.weights\n",
    "            bias = self.bias\n",
    "\n",
    "        if np.isscalar(inputs):\n",
    "            if self.data_dimension != 1:\n",
    "                raise ValueError(\"Mismatch between expected feature vector \"\n",
    "                    + f\"dimension ({self.data_dimension = }) and input \" \n",
    "                    + f\"shape ({np.shape(inputs) = }).\")\n",
    "            else:\n",
    "                return self.activation_function(\n",
    "                    SingleNeuron.preactivation(inputs, weights, bias))\n",
    "\n",
    "        elif inputs.shape == (self.data_dimension,):\n",
    "            return self.activation_function(\n",
    "                SingleNeuron.preactivation(inputs, weights, bias))\n",
    "        \n",
    "        elif inputs.shape[-1] == self.data_dimension    \\\n",
    "                or (self.data_dimension == 1 and inputs.ndim == 1):\n",
    "            return [self.activation_function(\n",
    "                        SingleNeuron.preactivation(input, weights, bias)) \n",
    "                    for input in inputs]\n",
    "\n",
    "        else: \n",
    "            raise ValueError(\"Mismatch between expected feature vector \"\n",
    "                + f\"dimension ({self.data_dimension = }) and input \" \n",
    "                + f\"shape ({np.shape(inputs) = }).\")\n",
    "    \n",
    "    def current_weights_and_bias(self):\n",
    "        \"\"\" \n",
    "        Utility function for wrapping weights and bias in a single tuple.\n",
    "        \"\"\"\n",
    "        return (self.weights.copy(), self.bias)\n",
    "    \n",
    "    def perceptron_stochastic_gradient_update(self, \n",
    "                                              input, \n",
    "                                              target_output):\n",
    "        \"\"\" \n",
    "        Updates the weights of the perceptron model using stochastic\n",
    "        descent. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input : array_like\n",
    "            The single input used to calculate the stochastic gradient at the\n",
    "            current weights.\n",
    "        \n",
    "        target_output : array_like\n",
    "            The single target output used to calculate the stochastic gradient\n",
    "            at the current weights.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        gradient : float\n",
    "            The calculated gradient used to update the weights. \n",
    "\n",
    "        \"\"\"\n",
    "        gradient = SingleNeuron.perceptron_stochastic_gradient(\n",
    "                        self.predict_outputs(input), target_output)\n",
    "        self.weights -= gradient * input\n",
    "        self.bias -= gradient\n",
    "        return gradient\n",
    "\n",
    "    def linear_regression_1D_stochastic_gradient_update(self, \n",
    "                                                        input, \n",
    "                                                        target_output, \n",
    "                                                        learning_rate, \n",
    "                                                        training_data_length):\n",
    "        \"\"\" \n",
    "        Updates the weights of the linear regression model using \n",
    "        stochastic descent.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input : array_like\n",
    "            The single input used to calculate the stochastic gradient at the\n",
    "            current weights.\n",
    "        \n",
    "        target_output : array_like\n",
    "            The single target output used to calculate the stochastic gradient\n",
    "            at the current weights.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        gradient : float\n",
    "            The calculated gradient used to update the weights.\n",
    "\n",
    "        \"\"\"\n",
    "        gradient = SingleNeuron.linear_regression_1D_stochastic_gradient(\n",
    "                                                self.predict_outputs(input), \n",
    "                                                target_output, \n",
    "                                                training_data_length)\n",
    "        self.weights -= learning_rate * gradient * input\n",
    "        self.bias -= learning_rate * gradient\n",
    "        return gradient\n",
    "        \n",
    "    def train(self, \n",
    "              inputs, \n",
    "              target_outputs, \n",
    "              learning_rate=0.5, \n",
    "              num_epochs=50):\n",
    "        \"\"\" \n",
    "        Trains the model (updates weights) using a batch of inputs and \n",
    "        target outputs.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : array_like\n",
    "            An array of feature vectors to train the model on. Must be as \n",
    "            many feature vectors as there are target outputs.\n",
    "        \n",
    "        target_outputs : array_like\n",
    "            An array of target outputs to compare the model's predictions to. \n",
    "            Must be as many targets as there are feature vectors.\n",
    "\n",
    "        learning_rate : float\n",
    "            Optional, and will not be used for the perceptron model; defines \n",
    "            the distance along the gradient by which the weights are adjusted \n",
    "            at every iteration.\n",
    "\n",
    "        num_epochs : int\n",
    "            Optional; defines the number of times the model looks at every \n",
    "            input-target pair. I.e. in each epoch the model looks at every\n",
    "            pair.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        loss_at_epoch : numpy.ndarray\n",
    "            An array of the value of the model's loss function at each epoch\n",
    "            in this training.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.previous_weights = np.copy(self.weights)\n",
    "        self.previous_bias = np.copy(self.bias)\n",
    "        weight_update = None\n",
    "        loss_function = None\n",
    "        \n",
    "        if self.model_type == SingleNeuron.type_perceptron:\n",
    "            weight_update = self.perceptron_stochastic_gradient_update\n",
    "            loss_function = SingleNeuron.perceptron_loss_function\n",
    "            \n",
    "        elif self.model_type == SingleNeuron.type_linear_regression_1D:\n",
    "            weight_update = lambda input, target : \\\n",
    "                self.linear_regression_1D_stochastic_gradient_update(\n",
    "                        input, target, learning_rate, len(target_outputs))\n",
    "            loss_function = SingleNeuron.linear_regression_loss_function\n",
    "\n",
    "        loss_at_epoch = np.empty(1 + num_epochs)\n",
    "        loss_at_epoch[0] = loss_function(self.predict_outputs(inputs),\n",
    "                                         target_outputs)\n",
    "\n",
    "        for epoch_index in ShadyBar(\n",
    "                \"Training\", \n",
    "                suffix=\"Epoch %(index)d / %(max)d\").iter(range(num_epochs)):\n",
    "            \n",
    "            for input, target_output in zip(inputs, target_outputs):\n",
    "                weight_update(input, target_output)\n",
    "                \n",
    "            loss_at_epoch[epoch_index+1] = loss_function(\n",
    "                    self.predict_outputs(inputs), target_outputs)\n",
    "\n",
    "        return loss_at_epoch\n",
    "\n",
    "    def reset_model(self):\n",
    "        \"\"\" \n",
    "        Randomizes the weights and bias of the model.\n",
    "        \"\"\"\n",
    "        if self.data_dimension == 1:\n",
    "            self.weights = np.random.randn()\n",
    "        else: \n",
    "            self.weights = np.random.randn(self.weights.shape)\n",
    "        self.bias = np.random.randn()\n",
    "\n",
    "    def forget_previous_training(self):\n",
    "        \"\"\" \n",
    "        Resets the weights and bias to their values before the most recent\n",
    "        training.\n",
    "        \"\"\"\n",
    "        self.weights = np.copy(self.previous_weights)\n",
    "        self.bias = np.copy(self.previous_bias)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Model type: \" + self.model_type + \"\\n\" \\\n",
    "            + \"Weights: \" + self.weights + \" | Bias: \" + self.bias\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sepal_length_inputs_2 = filtered_iris['sepal_length'][filtered_iris['species'] == -1]\n",
    "target_sepal_width_outputs_2 = filtered_iris['sepal_width'][filtered_iris['species'] == -1]\n",
    "linreg_test_2 = SingleNeuron(1, model_type=SingleNeuron.type_linear_regression_1D)\n",
    "linreg_test_2.train(sepal_length_inputs_2, target_sepal_width_outputs_2, num_epochs=200)\n",
    "plt.figure(figsize = (10, 7))\n",
    "plt.scatter(sepal_length_inputs_2, \n",
    "            target_sepal_width_outputs_2, \n",
    "            color = \"magenta\",\n",
    "            label = \"versicolor flowers\")\n",
    "plt.xlabel(\"sepal length [cm]\", fontsize = 15)\n",
    "plt.ylabel(\"sepal width [cm]\", fontsize = 15)\n",
    "plt.legend(fontsize = 15)\n",
    "plt.title(\"My Regression Data\", fontsize = 18)\n",
    "x = np.linspace(sepal_length_inputs_2.min(), sepal_length_inputs_2.max())\n",
    "plt.plot(x, linreg_test_2.weights*x + linreg_test_2.bias)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sepal_length_inputs = filtered_iris['sepal_length'][filtered_iris['species'] == 1].to_numpy()\n",
    "target_sepal_width_outputs = filtered_iris['sepal_width'][filtered_iris['species'] == 1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_test = SingleNeuron(1, model_type=SingleNeuron.type_linear_regression_1D)\n",
    "print(f\"{linreg_test.current_weights_and_bias() = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_test.reset_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 7))\n",
    "plt.scatter(sepal_length_inputs, \n",
    "            target_sepal_width_outputs, \n",
    "            color = \"lightseagreen\",\n",
    "            label = \"setosa flowers\")\n",
    "plt.xlabel(\"sepal length [cm]\", fontsize = 15)\n",
    "plt.ylabel(\"sepal width [cm]\", fontsize = 15)\n",
    "plt.legend(fontsize = 15)\n",
    "plt.title(\"My Regression Data\", fontsize = 18)\n",
    "x = np.linspace(sepal_length_inputs.min(), sepal_length_inputs.max())\n",
    "plt.plot(x, linreg_test.weights*x + linreg_test.bias)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_test.train(sepal_length_inputs, target_sepal_width_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_update0 = linreg_test.linear_regression_1D_stochastic_gradient_update(sepal_length_inputs[0], target_sepal_width_outputs[0], 0.5, 1)\n",
    "print(f\"{grad_update0 = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad0 = SingleNeuron.linear_regression_1D_stochastic_gradient(linreg_test.predict_outputs(sepal_length_inputs[0]), target_sepal_width_outputs[0], 1)\n",
    "print(f\"{grad0 = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss0 = SingleNeuron.linear_regression_loss_function(linreg_test.predict_outputs(sepal_length_inputs[0]), target_sepal_width_outputs[0])\n",
    "print(f\"{loss0 = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_test.predict_outputs(sepal_length_inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"{np.isscalar(sepal_length_inputs[0]) = }\")\n",
    "print(f\"{linreg_test.data_dimension = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{np.shape(sepal_length_inputs[0]) = }\")\n",
    "print(f\"{np.shape(5) = }\")\n",
    "print(f\"{np.shape(linreg_test.weights) = }\")\n",
    "print(f\"{np.isscalar(linreg_test.weights)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{np.shape(sepal_length_inputs[0]) = }\")\n",
    "print(f\"{np.shape([sepal_length_inputs[0]]) = }\")\n",
    "print(f\"{np.shape(linreg_test.weights) = }\")\n",
    "print(f\"{np.shape([linreg_test.weights]) = }\")\n",
    "print(f\"{np.shape(sepal_size_inputs[0]) = }\")\n",
    "print(f\"{np.shape([sepal_size_inputs[0]]) = }\")\n",
    "print(f\"{np.shape(model.weights) = }\")\n",
    "print(f\"{np.shape([model.weights]) = }\")\n",
    "print(f\"{np.shape(np.random.randn(2)) = }\")\n",
    "print(f\"{np.shape(np.asarray(sepal_length_inputs[0])) = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SingleNeuron(sepal_size_inputs.shape[1], model_type=\"perceptron\")\n",
    "model.current_weights_and_bias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.perceptron_stochastic_gradient_update(sepal_size_inputs[4], target_species_output[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(sepal_size_inputs, target_species_output, num_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.perceptron_stochastic_gradient_update(sepal_size_inputs[-1], target_species_output[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{model.predict_outputs(sepal_size_inputs[0]) = }, but  {target_species_output[0] = }\")\n",
    "gradient = SingleNeuron.perceptron_stochastic_gradient(model.predict_outputs(sepal_size_inputs[0]), target_species_output[0])\n",
    "print(f\"{gradient = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_model()\n",
    "model.current_weights_and_bias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_outputs = model.predict_outputs(sepal_size_inputs)\n",
    "print(f\"{SingleNeuron.perceptron_loss_function(predicted_outputs, target_species_output) = }\")\n",
    "#hmm... why is this consistently outputting 50...\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(sepal_size_inputs[target_species_output == 1, 0], \n",
    "            sepal_size_inputs[target_species_output == 1, 1], \n",
    "            color=\"magenta\", label=\"Setosa (1)\")\n",
    "plt.scatter(sepal_size_inputs[target_species_output == -1, 0], \n",
    "            sepal_size_inputs[target_species_output == -1, 1], \n",
    "            color=\"green\", label=\"Versicolor (-1)\")\n",
    "x = np.linspace(sepal_size_inputs[:,0].min(), sepal_size_inputs[:,0].max())\n",
    "plt.plot(x, (-model.weights[0]*x - model.bias) / model.weights[1]) \n",
    "\n",
    "#Oh if it's frequently outputting 50 because it's getting 50 values wrong most of the time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{model.predict_outputs(sepal_size_inputs[0]) = }\")\n",
    "print(f\"{model.predict_outputs(sepal_size_inputs[0:3]) = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preactivation_value = SingleNeuron.preactivation(sepal_size_inputs[0], model.weights, model.bias)\n",
    "print(f\"{preactivation_value = }\")\n",
    "print(f\"{model.activation_function(preactivation_value) = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{[sepal_size for sepal_size in sepal_size_inputs[0,:]] = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sepal_size_inputs[0].ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{SingleNeuron.preactivation(sepal_size_inputs[0], model.weights, model.bias) = }\")\n",
    "print(f\"{np.dot(sepal_size_inputs[0], model.weights) + model.bias = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{SingleNeuron.sign(-0.00001) = }\")\n",
    "print(f\"{SingleNeuron.linear_1D(523/100) = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sepal_size_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sepal_size_inputs[target_species_output==1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{model.current_weights_and_bias() = }\")\n",
    "[current_weights, current_bias] = model.current_weights_and_bias()\n",
    "print(f\"{sepal_size_inputs[1] = }\")\n",
    "print(f\"{SingleNeuron.preactivation(sepal_size_inputs[1], current_weights, current_bias) = }\")\n",
    "preactivation_value = SingleNeuron.preactivation(sepal_size_inputs[1], current_weights, current_bias)\n",
    "print(f\"{model.activation_function(preactivation_value) = }\")\n",
    "print(f\"{model.predict_outputs(sepal_size_inputs[1]) = }\")\n",
    "inputs = sepal_size_inputs\n",
    "activation_values = [model.activation_function(SingleNeuron.preactivation(input, current_weights, current_bias)) for input in inputs]\n",
    "print(f\"{activation_values = }\")\n",
    "print(f\"{model.predict_outputs(inputs) = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(SingleNeuron.sign(SingleNeuron.preactivation(sepal_size_inputs[1], model.weights, model.bias)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{sepal_size_inputs[1] = }\")\n",
    "print(f\"{np.shape(sepal_size_inputs[1]) = } but {np.shape(model.weights) = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SingleNeuron.sign(np.float64(3.3034947687319463))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{SingleNeuron.sign(0) = }\")\n",
    "print(f\"{SingleNeuron.linear_1D(8.3) = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{sepal_size_inputs.shape = }\")\n",
    "print(f\"{sepal_size_inputs.shape[1] = }\")\n",
    "print(f\"{sepal_size_inputs[1] = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(3)\n",
    "type(x) == np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.random.randn(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xval in x:\n",
    "    print(xval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((2,2))\n",
    "b = np.copy(a)\n",
    "a[0,0] = 1\n",
    "a[1,0] = 2\n",
    "print(a)\n",
    "print(b)\n",
    "print(np.copy(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([A for A in a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CMOR438_base1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
