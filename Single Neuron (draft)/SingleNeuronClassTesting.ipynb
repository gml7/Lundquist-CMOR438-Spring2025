{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "#234567891123456789212345678931234567894123456789512345678961234567897123456789\n",
    "#                                python docstring limit: 72 characters |      \n",
    "#                                            python code limit: 79 characters |\n",
    "# test change\n",
    "class SingleNeuron(object):\n",
    "    \"\"\"\n",
    "    A class used to represent a single neuron (in the machine learning \n",
    "    sense).\n",
    "\n",
    "    Design notes: the model type and the activation function are \n",
    "    intended to be immutable, though of course python still lets you mutate them.\n",
    "    \"\"\"\n",
    "    type_perceptron = \"perceptron\"\n",
    "    type_linear_regression_1D = \"linear regression 1D\"\n",
    "\n",
    "    def sign(input_value):\n",
    "        \"\"\" \n",
    "\n",
    "        \"\"\"\n",
    "        if input_value >= 0:\n",
    "            return 1\n",
    "        else: \n",
    "            return 0\n",
    "        \n",
    "    def linear_1D(input_value):\n",
    "        \"\"\" \n",
    "\n",
    "        \"\"\"\n",
    "        return input_value\n",
    "    \n",
    "    # default_activation_function = sign\n",
    "\n",
    "    # def test_for_matching_ndarrays(var_1, var_2):\n",
    "    #     return type(var_1) != np.ndarray or type(var_2) != np.ndarray \\\n",
    "    #            or var_1.shape != var_1.shape\n",
    "\n",
    "    def perceptron_loss_function(predicted_outputs, target_outputs):\n",
    "        \"\"\" \n",
    "        \n",
    "        \"\"\"\n",
    "        # if SingleNeuron.test_for_matching_ndarrays(predicted_values, true_values) == False:\n",
    "        #     raise TypeError\n",
    "        return (1/4) * np.sum((predicted_outputs - target_outputs)**2)\n",
    "    \n",
    "    def perceptron_stochastic_gradient(predicted_output, target_output):\n",
    "        \"\"\" \n",
    "        \n",
    "        \"\"\"\n",
    "        return (1/2) * (predicted_output - target_output)\n",
    "\n",
    "    def linear_regression_loss_function(predicted_outputs, target_outputs):\n",
    "        \"\"\" \n",
    "        \n",
    "        \"\"\"\n",
    "        # if SingleNeuron.test_for_matching_ndarrays(predicted_values, true_values) == False:\n",
    "        #     raise TypeError\n",
    "        return (1/(2*target_outputs.size)) * np.sum((predicted_outputs - target_outputs)**2)\n",
    "        \n",
    "    def linear_regression_1D_stochastic_gradient(predicted_output, target_output, training_data_length):\n",
    "        \"\"\" \n",
    "        \n",
    "        \"\"\"\n",
    "        return (1/training_data_length) * (predicted_output - target_output)\n",
    "\n",
    "    def preactivation(input, weights, bias):\n",
    "        \"\"\" \n",
    "        \n",
    "        \"\"\"\n",
    "        return np.dot(input, weights) + bias\n",
    "    \n",
    "    def __init__(self, data_dimension, model_type, weights=None, bias=None, \n",
    "                 activation_function=None):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        if activation_function == None:\n",
    "            if self.model_type == SingleNeuron.type_perceptron:\n",
    "                self.activation_function = SingleNeuron.sign\n",
    "            elif self.model_type == SingleNeuron.type_linear_regression_1D:\n",
    "                self.activation_function = SingleNeuron.linear_1D\n",
    "        else:\n",
    "            self.activation_function = activation_function\n",
    "        \n",
    "        if weights == None:\n",
    "            self.weights = np.random.randn(data_dimension)\n",
    "        else:\n",
    "            self.weights = weights\n",
    "\n",
    "        if bias == None:\n",
    "            self.bias = np.random.randn()\n",
    "        else:\n",
    "            self.bias = bias\n",
    "\n",
    "    def predict_outputs(self, inputs, weights=None, bias=None, use_current_weights_and_bias=True):\n",
    "        \"\"\" \n",
    "        \n",
    "        \"\"\"\n",
    "        if use_current_weights_and_bias:\n",
    "            weights = self.weights\n",
    "            bias = self.bias\n",
    "        if np.shape(inputs)[0] == 1:\n",
    "            warnings.warn(\"Input's first dimension is 1; defaulting to iterating on second dimension\")\n",
    "        return [self.activation_function(SingleNeuron.preactivation(input, weights, bias)) for input in inputs]\n",
    "    \n",
    "    def current_weights(self):\n",
    "        \"\"\" \n",
    "        \n",
    "        \"\"\"\n",
    "        return self.weights.copy()\n",
    "    \n",
    "    def current_bias(self):\n",
    "        \"\"\" \n",
    "        \n",
    "        \"\"\"\n",
    "        return self.bias\n",
    "\n",
    "    def current_weights_and_bias(self):\n",
    "        \"\"\" \n",
    "        \n",
    "        \"\"\"\n",
    "        return (self.weights.copy(), self.bias)\n",
    "    \n",
    "    def perceptron_stochastic_gradient_update(self, input, target_output, learning_rate=None):\n",
    "        \"\"\" \n",
    "        learning_rate does nothing and is just for making it more uniform to pass around update functions\n",
    "        \"\"\"\n",
    "        gradient = SingleNeuron.perceptron_stochastic_gradient(self.predict_outputs(input), target_output)\n",
    "        self.weights -= gradient * input\n",
    "        self.bias -= gradient\n",
    "        return gradient\n",
    "\n",
    "    def linear_regression_1D_stochastic_gradient_update(self, \n",
    "                input, target_output, learning_rate):\n",
    "        gradient = SingleNeuron.linear_regression_1D_stochastic_gradient(self.predict_outputs(input), target_output)\n",
    "        self.weights -= learning_rate * gradient * input\n",
    "        self.bias -= learning_rate * gradient\n",
    "        return gradient\n",
    "        \n",
    "    def train(self, inputs, target_outputs, learning_rate=0.5, num_epochs=50):\n",
    "        \"\"\" \n",
    "        \n",
    "        \"\"\"\n",
    "        # if self.model_type != SingleNeuron.type_perceptron and (learning_rate == None or epochs == None):\n",
    "        #     raise ValueError(\"learning_rate and epochs must be specified for non-perceptron models\")\n",
    "        weight_bias_update = None\n",
    "        loss_function = None\n",
    "        if self.model_type == SingleNeuron.type_perceptron:\n",
    "            weight_bias_update = self.perceptron_stochastic_gradient_update\n",
    "            loss_function = SingleNeuron.perceptron_loss_function\n",
    "        elif self.model_type == SingleNeuron.type_linear_regression_1D:\n",
    "            weight_bias_update = self.linear_regression_1D_stochastic_gradient_update\n",
    "            loss_function = SingleNeuron.linear_regression_loss_function\n",
    "\n",
    "        loss_at_epoch = [-1] * (num_epochs + 1)\n",
    "        loss_at_epoch[0] = loss_function(self.predict_outputs(inputs), target_outputs)\n",
    "\n",
    "        for epoch_index in range(num_epochs):\n",
    "            for input, target_output in zip(inputs, target_outputs):\n",
    "                weight_bias_update(input, target_output, learning_rate)\n",
    "            loss_at_epoch[epoch_index+1] = loss_function(self.predict_outputs(inputs), target_outputs)\n",
    "\n",
    "# todo\n",
    "    def __repr__(self):\n",
    "        \"\"\" \n",
    "        \n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "# might not be necessary\n",
    "    # def __call__(self):\n",
    "    #     return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "# Load the iris dataset from seaborn\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "\n",
    "# Filter the dataset to only include 'versicolor' and 'setosa' species\n",
    "filtered_iris = iris[iris['species'].isin(['versicolor', 'setosa'])]\n",
    "\n",
    "# Select only 'sepal_length' and 'sepal_width' variables\n",
    "filtered_iris = filtered_iris[['sepal_length', 'sepal_width', 'species']]\n",
    "\n",
    "# Label 'setosa' as 1 and 'versicolor' as -1\n",
    "filtered_iris['species'] = filtered_iris['species'].map({'setosa': 1, 'versicolor': -1})\n",
    "\n",
    "sepal_size_inputs = filtered_iris[['sepal_length', 'sepal_width']].to_numpy()\n",
    "target_species_output = filtered_iris['species'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_test = SingleNeuron(sepal_size_inputs.shape[1], \"perceptron\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuron_test.current_weights_and_bias() = (array([0.60521726, 1.08718608]), -0.39798925601588264)\n",
      "sepal_size_inputs[1] = array([4.9, 3. ])\n",
      "SingleNeuron.preactivation(sepal_size_inputs[1], current_weights, current_bias) = np.float64(5.829133563528094)\n",
      "neuron_test.activation_function(preactivation_value) = 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m preactivation_value \u001b[38;5;241m=\u001b[39m SingleNeuron\u001b[38;5;241m.\u001b[39mpreactivation(sepal_size_inputs[\u001b[38;5;241m1\u001b[39m], current_weights, current_bias)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mneuron_test\u001b[38;5;241m.\u001b[39mactivation_function(preactivation_value)\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m= }\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mneuron_test\u001b[38;5;241m.\u001b[39mpredict_outputs(sepal_size_inputs[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[37], line 106\u001b[0m, in \u001b[0;36mSingleNeuron.predict_outputs\u001b[1;34m(self, inputs, weights, bias, use_current_weights_and_bias)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mshape(inputs)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    105\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms first dimension is 1; defaulting to iterating on second dimension\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_function(SingleNeuron\u001b[38;5;241m.\u001b[39mpreactivation(\u001b[38;5;28minput\u001b[39m, weights, bias)) \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs]\n",
      "Cell \u001b[1;32mIn[37], line 22\u001b[0m, in \u001b[0;36mSingleNeuron.sign\u001b[1;34m(input_value)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msign\u001b[39m(input_value):\n\u001b[0;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" \u001b[39;00m\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_value \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: \n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "print(f\"{neuron_test.current_weights_and_bias() = }\")\n",
    "[current_weights, current_bias] = neuron_test.current_weights_and_bias()\n",
    "print(f\"{sepal_size_inputs[1] = }\")\n",
    "print(f\"{SingleNeuron.preactivation(sepal_size_inputs[1], current_weights, current_bias) = }\")\n",
    "preactivation_value = SingleNeuron.preactivation(sepal_size_inputs[1], current_weights, current_bias)\n",
    "print(f\"{neuron_test.activation_function(preactivation_value) = }\")\n",
    "print(f\"{neuron_test.predict_outputs(sepal_size_inputs[1])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m [(SingleNeuron\u001b[38;5;241m.\u001b[39msign(SingleNeuron\u001b[38;5;241m.\u001b[39mpreactivation(\u001b[38;5;28minput\u001b[39m, neuron_test\u001b[38;5;241m.\u001b[39mcurrent_weights(), neuron_test\u001b[38;5;241m.\u001b[39mcurrent_bias()))) \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sepal_size_inputs[\u001b[38;5;241m1\u001b[39m]]\n",
      "Cell \u001b[1;32mIn[31], line 21\u001b[0m, in \u001b[0;36mSingleNeuron.sign\u001b[1;34m(input_value)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msign\u001b[39m(input_value):\n\u001b[0;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" \u001b[39;00m\n\u001b[0;32m     19\u001b[0m \n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_value \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: \n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "[(SingleNeuron.sign(SingleNeuron.preactivation(input, neuron_test.current_weights(), neuron_test.current_bias()))) for input in sepal_size_inputs[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SingleNeuron.sign(np.float64(3.3034947687319463))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SingleNeuron.sign(0) = 1\n",
      "SingleNeuron.linear_1D(8.3) = 8.3\n"
     ]
    }
   ],
   "source": [
    "print(f\"{SingleNeuron.sign(0) = }\")\n",
    "print(f\"{SingleNeuron.linear_1D(8.3) = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal_size_inputs.shape = (100, 2)\n",
      "sepal_size_inputs.shape[1] = 2\n",
      "sepal_size_inputs[1] = array([4.9, 3. ])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{sepal_size_inputs.shape = }\")\n",
    "print(f\"{sepal_size_inputs.shape[1] = }\")\n",
    "print(f\"{sepal_size_inputs[1] = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randn(3)\n",
    "type(x) == np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.random.randn(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (3,) and (4,) not aligned: 3 (dim 0) != 4 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Users\\gabri\\Documents\\Data Science & Machine Learning Spring 2025\\Lundquist-CMOR438-Spring2025\\Single Neuron (draft)\\SingleNeuronClassTesting.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Users/gabri/Documents/Data%20Science%20%26%20Machine%20Learning%20Spring%202025/Lundquist-CMOR438-Spring2025/Single%20Neuron%20%28draft%29/SingleNeuronClassTesting.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m np\u001b[39m.\u001b[39mdot(x,y)\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (3,) and (4,) not aligned: 3 (dim 0) != 4 (dim 0)"
     ]
    }
   ],
   "source": [
    "np.dot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5437932433939918\n",
      "-1.1348160642001053\n",
      "0.3022609783103276\n"
     ]
    }
   ],
   "source": [
    "for xval in x:\n",
    "    print(xval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CMOR438_env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
