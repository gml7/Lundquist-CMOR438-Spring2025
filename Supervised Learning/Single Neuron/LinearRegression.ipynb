{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d72f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.activate(\"..\\\\..\\\\juMLia\")\n",
    "import MLDatasets: Wine\n",
    "using Plots, DataFrames, JSON3\n",
    "wine = Wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8de2a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "linregdata = subset(wine.dataframe, :Wine => x -> x .== 3)[:, [:OD, :Proanth]] # Only select cultivar 3\n",
    "scatter(linregdata[:, :OD], linregdata[:, :Proanth])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9e0d24",
   "metadata": {},
   "source": [
    "We select some data from the Wine dataset to perform linear regression on. It looks okay! There are a few linear-looking features that a regression algorithm might grab on to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7da534",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"SingleNeuron.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0729c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "linregmodel = SingleNeuron(1, :linearregression)\n",
    "plotneuron(linregmodel; leftbound=1.2, rightbound=2.4)\n",
    "scatter!(linregdata[:, :OD], linregdata[:, :Proanth])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f119eb0",
   "metadata": {},
   "source": [
    "The linear regression model is very similar to the perceptron, except for a key difference: rather than using a binary activation function, it just spits out the value it gets from summing the product of the input and the weights. That's right: preactivation is a linear combination of the elements of the feature vector!\n",
    "\n",
    "The loss function that linear regression travels down is similar to that of the perceptron in that it is proportional to the euclidean norm of the difference between the predicted value and the actual value. This makes the gradient similarly simple: it's just the difference between the predicted value and the actual value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06128f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train!(linregmodel, linregdata[:, :OD], linregdata[:, :Proanth], learningrate=0.05, numepochs=10)\n",
    "plotneuron(linregmodel; leftbound=1.2, rightbound=2.4)\n",
    "scatter!(linregdata[:, :OD], linregdata[:, :Proanth])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d8447c",
   "metadata": {},
   "source": [
    "Well, unfortunately our regression didn't latch onto any of the linear features in particular. You can see why it would make sense: it is trying to take into account both the points on the lower right of the plot as well as the top of the plot, so ends up modeling neither. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
