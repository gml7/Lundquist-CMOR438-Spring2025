{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acb7aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.activate(\"..\\\\..\\\\juMLia\")\n",
    "import MLDatasets: FashionMNIST, convert2image\n",
    "using Plots, DataFrames, ColorSchemes, ImageShow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d44334",
   "metadata": {},
   "source": [
    "Neural networks are overkill for datasets on the scale of Wine. But they're useful in things like computer vision here, where we're classifying tiny images of different fashion items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e617943",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = FashionMNIST(:train)\n",
    "testdata = FashionMNIST(:test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9da0622",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert2image(traindata, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a89dcb",
   "metadata": {},
   "source": [
    "We're going to use the sigmoid function as the activation function for all the neurons in the network, so we need a way to get a value between 0 and 9 out of a bunch of binary classifiers. We do it by encoding the value as a bit sequence, where the index of the only on bit corresponds to the value. When we get an output from the network, we'll call argmax on the float values we get from it to get the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae39ef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "function onehotencodedigits(digit::Int)\n",
    "    digitencoded = BitArray(zeros(10))\n",
    "    digitencoded[digit+1] = 1\n",
    "    return digitencoded\n",
    "end\n",
    "\n",
    "\"One-hot encodes a vector of integers 0-9.\"\n",
    "function onehotencodedigits(digitlist::Vector{<:Int})\n",
    "    return onehotencodedigits.(digitlist)\n",
    "end\n",
    "\n",
    "function onehotdecodedigits(digitsencoded::BitArray)\n",
    "    return argmax(digitsencoded) - 1\n",
    "end\n",
    "\n",
    "function onehotdecodedigits(digitsencoded::Vector{<:BitArray})\n",
    "    return onehotdecodedigits.(digitsencoded)\n",
    "end\n",
    "\n",
    "function onehotdecodedigits(digitsencoded::Vector{<:Number})\n",
    "    return argmax(digitsencoded) - 1\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b4eb5d",
   "metadata": {},
   "source": [
    "One simplification we make is we flatten the 2D image into one-dimensional vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcc83bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfeatures = vec.(traindata.features[:,:,ftridx] for ftridx in 1:size(traindata.features, 3))\n",
    "trainlabels = onehotencodedigits.(traindata.targets)\n",
    "\n",
    "testfeatures = vec.(testdata.features[:,:,ftridx] for ftridx in 1:size(testdata.features, 3))\n",
    "testlabels = onehotencodedigits.(testdata.targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e38d6e3",
   "metadata": {},
   "source": [
    "We descend down the loss function according to the gradient defined by the first derivative of the sigmoid function (which happens to be nice and closed-form)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6097687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(value) = 1.0 ./ (1.0 .+ exp.(-value))\n",
    "\n",
    "sigmoid_firstderiv(value) = sigmoid(value) .* (1.0 .- sigmoid(value))\n",
    "\n",
    "\"\"\"Mean squared error between `predictions` and `targets`. Used as a \n",
    "loss function.\"\"\"\n",
    "meansquarederror(predictions, targets) = 0.5 * sum((predictions .- targets).^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76257c8",
   "metadata": {},
   "source": [
    "We implement a dense, deep neural network, where every neuron at a particular layer is connected to every neuron at the following layer and there are at least two hidden layers (hidden meaning between the input and output layers). Generally, each layer might have its own activation functions and gradient functions. Because of the density of the network, we store the weights as matrices--going from one layer to the next corresponds to multiplying the output vector of one layer by the weight matrix and adding the bias vector, then applying the activation function to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f97c9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type DenseNeuralNetwork end\n",
    "\n",
    "# Struct of arrays\n",
    "mutable struct DenseNeuralNetworkSoA{LossF} <: DenseNeuralNetwork\n",
    "    const activationfunctions::Vector{Function}\n",
    "    const gradients::Vector{Function}\n",
    "    const loss::LossF\n",
    "    weights::Vector{Matrix{Float64}} # Can't store as array because matrices may have different sizes\n",
    "    bias::Vector{Vector{Float64}}\n",
    "    previousweights::Vector{Matrix{Float64}}\n",
    "    previousbias::Vector{Vector{Float64}}\n",
    "    losshistory::Vector{Float64}\n",
    "    prevlosshistory::Vector{Float64}\n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026b8b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "function initweights(layerlengths; randweights = true)\n",
    "    numweightmatrices = length(layerlengths) - 1\n",
    "    weights = Vector{Matrix{Float64}}(undef, numweightmatrices)\n",
    "    bias = Vector{Vector{Float64}}(undef, numweightmatrices)\n",
    "    for layerindex in 1:numweightmatrices\n",
    "        if randweights\n",
    "            weights[layerindex] = (sqrt(2 / layerlengths[layerindex]) \n",
    "                                    .* randn((layerlengths[layerindex+1], \n",
    "                                                layerlengths[layerindex])))\n",
    "            \n",
    "            bias[layerindex] = (sqrt(2 / layerlengths[layerindex])\n",
    "                                .* randn(layerlengths[layerindex+1]))\n",
    "        else\n",
    "            weights[layerindex] = zeros(layerlengths[layerindex+1], \n",
    "                                        layerlengths[layerindex])\n",
    "            bias[layerindex] = zeros(layerlengths[layerindex+1])\n",
    "        end\n",
    "    end\n",
    "    return (weights, bias)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b275d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "function DenseNeuralNetworkSoA(activationfunctions::Vector,\n",
    "    gradientfunctions::Vector,\n",
    "    lossfunction,\n",
    "    weights::Vector{Matrix{Float64}},\n",
    "    bias::Vector{Vector{Float64}})\n",
    "    if length(activationfunctions) != length(gradientfunctions)\n",
    "        error(\"Function lists must be of the same length. \\\n",
    "        Given lengths are $(length(activationfunctions)) \\\n",
    "        and $(length(gradientfunctions)).\")\n",
    "    end\n",
    "\n",
    "    if length(weights) != length(bias)\n",
    "        error(\"Weights and bias vectors must be of compatible length. \\\n",
    "        Given lengths are $(length(weights)) \\\n",
    "        and $(length(bias)).\")\n",
    "    end\n",
    "\n",
    "    if length(weights) != length(activationfunctions)\n",
    "        error(\"Number of weight matrices and number of functions \\\n",
    "        be the same. Given are $(length(weights)) \\\n",
    "        weights and $(length(activationfunctions)) \\\n",
    "        functions.\")\n",
    "    end\n",
    "\n",
    "    return DenseNeuralNetworkSoA{typeof(lossfunction)}(activationfunctions,\n",
    "        gradientfunctions,\n",
    "        lossfunction,\n",
    "        weights, bias, weights, bias,\n",
    "        [], [])\n",
    "end\n",
    "\n",
    "function DenseNeuralNetworkSoA(activationfunctions::Vector,\n",
    "    gradientfunctions::Vector,\n",
    "    lossfunction,\n",
    "    numnodesperlayer)\n",
    "\n",
    "    numlayers = length(activationfunctions) + 1\n",
    "\n",
    "    if (length(numnodesperlayer) != numlayers\n",
    "        &&\n",
    "        length(numnodesperlayer) > 1)\n",
    "        error(\"Must be either a constant node count or as many \n",
    "        node counts as there are layers. \\\n",
    "        Given layer count is $(numlayers) \\\n",
    "        and the node counts specified are of length $(length(numnodesperlayer)).\")\n",
    "    end\n",
    "\n",
    "    weights = nothing\n",
    "    bias = nothing\n",
    "    if isa(numnodesperlayer, <:Int)\n",
    "        (weights, bias) = initweights(fill(numnodesperlayer, numlayers))\n",
    "    else\n",
    "        (weights, bias) = initweights(numnodesperlayer)\n",
    "    end\n",
    "\n",
    "    return DenseNeuralNetworkSoA(activationfunctions, gradientfunctions,\n",
    "        lossfunction,\n",
    "        weights, bias, weights, bias,\n",
    "        [], [])\n",
    "end\n",
    "\n",
    "function DenseNeuralNetworkSoA(activationfunction,\n",
    "    gradientfunction,\n",
    "    lossfunction,\n",
    "    weights::Vector{Matrix{Float64}},\n",
    "    bias::Vector{Vector{Float64}})\n",
    "\n",
    "    numweightmatrices = length(weights)\n",
    "    return DenseNeuralNetworkSoA(fill(activationfunction, numweightmatrices),\n",
    "        fill(gradientfunction, numweightmatrices),\n",
    "        lossfunction, weights, bias)\n",
    "end\n",
    "\n",
    "function DenseNeuralNetworkSoA(activationfunction,\n",
    "    gradientfunction,\n",
    "    lossfunction,\n",
    "    numnodesperlayer::Vector{<:Int})\n",
    "\n",
    "    numlayers = length(numnodesperlayer)\n",
    "    return DenseNeuralNetworkSoA(fill(activationfunction, numlayers - 1),\n",
    "        fill(gradientfunction, numlayers - 1),\n",
    "        lossfunction, weights, bias)\n",
    "end\n",
    "\n",
    "function DenseNeuralNetworkSoA(weights::Vector{Matrix{Float64}},\n",
    "    bias::Vector{Vector{Float64}})\n",
    "    return DenseNeuralNetworkSoA(sigmoid, sigmoid_firstderiv, meansquarederror,\n",
    "        weights, bias)\n",
    "end\n",
    "\n",
    "function DenseNeuralNetworkSoA(inputlength::Int,\n",
    "    outputlength::Int;\n",
    "    numhiddenlayers::Int=2,\n",
    "    hiddenlayerlengths::Vector{<:Int}=\n",
    "    fill(round(Int, √(inputlength * outputlength)),\n",
    "        numhiddenlayers)\n",
    ")\n",
    "    numweightmatrices = 1 + numhiddenlayers\n",
    "    if length(hiddenlayerlengths) != numhiddenlayers\n",
    "        error(\"`hiddenlayerlengths` vector must have $(numhiddenlayers) \\\n",
    "        entries.\")\n",
    "    end\n",
    "    layerlengths = [inputlength; hiddenlayerlengths; outputlength]\n",
    "    return DenseNeuralNetworkSoA(initweights(layerlengths)...)\n",
    "end\n",
    "\n",
    "function DenseNeuralNetworkSoA(nodecounts::Vector{<:Int})\n",
    "    return DenseNeuralNetworkSoA(initweights(nodecounts)...)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85462bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "function numlayers(neurnet::DenseNeuralNetworkSoA)\n",
    "    return length(neurnet.weights) + 1\n",
    "end\n",
    "\n",
    "function weightsizes(neurnet::DenseNeuralNetworkSoA)\n",
    "    return [size(weightmat) for weightmat in neurnet.weights]\n",
    "end\n",
    "\n",
    "function numnodesinlayer(neurnet::DenseNeuralNetworkSoA, layerindex::Int)\n",
    "    if layerindex <= length(neurnet.weights)\n",
    "        return size(neurnet.weights[layerindex])[end]\n",
    "    elseif layerindex == length(neurnet.weights) + 1\n",
    "        return size(neurnet.weights[layerindex])[begin]\n",
    "    else\n",
    "        return NaN\n",
    "    end\n",
    "end\n",
    "\n",
    "function numnodesalllayers(neurnet::DenseNeuralNetworkSoA)\n",
    "    return [size(neurnet.weights[begin])[end]; \n",
    "            [size(weightmat)[begin] for weightmat in neurnet.weights]]\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5fed87",
   "metadata": {},
   "source": [
    "A forward pass constitutes a single run through the network: you give it a single feature vector, the network multiplies through its weight matrices and passes the results through its activation functions, then the network outputs a label vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c117c4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "function forwardpass(neurnet::DenseNeuralNetworkSoA, input)\n",
    "    output = input\n",
    "    for (weightmat, biasvec, activate) in zip(neurnet.weights, \n",
    "                                              neurnet.bias, \n",
    "                                              neurnet.activationfunctions)\n",
    "        # Creating all these intermediate vectors of potentially very large size...\n",
    "        # ...is fine! Happens very quickly and is actually a little faster than \n",
    "        # copying into an already-existing array.\n",
    "        output = activate(weightmat * output .+ biasvec)\n",
    "    end\n",
    "    return output\n",
    "end\n",
    "\n",
    "function forwardpass_keepvals!(neurnet::DenseNeuralNetworkSoA, input, \n",
    "                                preactivationvecs::Array{T}, \n",
    "                                postactivationvecs) where T <: Array{U} where U\n",
    "    output = postactivationvecs[1] = input\n",
    "    preactivationvecs[1] = [zero(eltype(eltype(preactivationvecs)))]\n",
    "\n",
    "    for (i, (weightmat, biasvec, activate)) in enumerate(zip(neurnet.weights, \n",
    "                                                                neurnet.bias, \n",
    "                                                                neurnet.activationfunctions))\n",
    "        preactivationvecs[i+1] = weightmat * output .+ biasvec\n",
    "        output = postactivationvecs[i+1] = activate(preactivationvecs[i+1])\n",
    "    end\n",
    "\n",
    "    return (output, preactivationvecs, postactivationvecs)\n",
    "end\n",
    "\n",
    "function forwardpass_keepvals(neurnet, input)\n",
    "    preactivationvecs = Vector{Vector{Float64}}(undef, numlayers(neurnet))\n",
    "    postactivationvecs = Vector{Vector{Float64}}(undef, numlayers(neurnet))\n",
    "\n",
    "    return forwardpass_keepvals!(neurnet, input, preactivationvecs, postactivationvecs)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb78cd1",
   "metadata": {},
   "source": [
    "The network predicting a value, then, only requires performing a forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b9212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "function predictsingle(neurnet, input)\n",
    "    return forwardpass(neurnet, input)\n",
    "end\n",
    "\n",
    "function predictmultiple(neurnet, inputs::Vector)\n",
    "    return [forwardpass(neurnet, input) for input in inputs]\n",
    "end\n",
    "\n",
    "function predictmultiple(neurnet, inputs::Matrix)\n",
    "    return [forwardpass(neurnet, view(inputs, :, colindex)) for colindex in 1:size(inputs, 2)]\n",
    "end\n",
    "\n",
    "function predictsingle_keepvals(neurnet, input)\n",
    "    return forwardpass_keepvals(neurnet, input)\n",
    "end\n",
    "\n",
    "function predictmultiple_keepvals(neurnet, inputs::Vector)\n",
    "    return [forwardpass_keepvals(neurnet, view(inputs, :, colindex)) for colindex in 1:size(inputs, 2)]\n",
    "end\n",
    "\n",
    "function predict(neurnet, input; keepvals=false)\n",
    "    if !keepvals\n",
    "        return predictsingle(neurnet, input)\n",
    "    else\n",
    "        return predictsingle_keepvals(neurnet, input)\n",
    "    end\n",
    "end\n",
    "\n",
    "# Generally if you give it a vector it assumes you want multiple predictions...\n",
    "function predict(neurnet, inputs::Vector; keepvals=false)\n",
    "    if !keepvals\n",
    "        return predictmultiple(neurnet, inputs)\n",
    "    else\n",
    "        return predictmultiple_keepvals(neurnet, inputs)\n",
    "    end\n",
    "end\n",
    "\n",
    "# ...but if it's a vector of numbers, it chooses between whether to treat \n",
    "# it as a vector of inputs or a single input based on the dimension of the\n",
    "# first layer in the network.\n",
    "function predict(neurnet, inputs::Vector{<:Number}; \n",
    "                    keepvals=false)\n",
    "    if numnodesinlayer(neurnet, 1) == 1\n",
    "        if !keepvals\n",
    "            return predictmultiple(neurnet, inputs)\n",
    "        else\n",
    "            return predictmultiple_keepvals(neurnet, inputs)\n",
    "        end\n",
    "    else \n",
    "        if !keepvals\n",
    "            return predictsingle(neurnet, inputs)\n",
    "        else\n",
    "            return predictsingle_keepvals(neurnet, inputs)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function predict(neurnet, inputs::Matrix; keepvals=false)\n",
    "    if !keepvals\n",
    "        return predictmultiple(neurnet, inputs)\n",
    "    else\n",
    "        return predictmultiple_keepvals(neurnet, inputs)\n",
    "    end\n",
    "end\n",
    "\n",
    "function predict(neurnet, inputs::DataFrame; keepvals=false)\n",
    "    return predict(neurnet, Vector.(eachrow(inputs)), keepvals)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf1e534",
   "metadata": {},
   "outputs": [],
   "source": [
    "function predictionerror(neurnet, inputs, targets)\n",
    "    numcomparisons = min(length(inputs), length(targets))\n",
    "    predictions = predictmultiple(neurnet, inputs[1:numcomparisons])\n",
    "    return sum(neurnet.loss.(predictions, targets[1:numcomparisons])) / numcomparisons\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79e1386",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Returns whether `inputs` and `targets` have the same length for training \n",
    "purposes. Not an exported function.\"\"\"\n",
    "function equaldatalengths(inputs, targets)\n",
    "    return length(inputs) == length(targets)\n",
    "end\n",
    "\n",
    "\"\"\"Returns whether `inputs` and `targets` have the same length for training \n",
    "purposes. Not an exported function.\"\"\"\n",
    "function equaldatalengths(inputs::DataFrame, targets)\n",
    "    return nrow(inputs) == length(targets)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618a6325",
   "metadata": {},
   "outputs": [],
   "source": [
    "function updateprevweights!(neurnet::DenseNeuralNetworkSoA)\n",
    "    neurnet.previousweights = deepcopy(neurnet.weights)\n",
    "    neurnet.previousbias = deepcopy(neurnet.bias)\n",
    "    return (neurnet.previousweights, neurnet.previousbias)\n",
    "end\n",
    "\n",
    "function forgetprevtraining!(neurnet::DenseNeuralNetworkSoA)\n",
    "    neurnet.weights = deepcopy(neurnet.previousweights)\n",
    "    neurnet.bias = deepcopy(neurnet.previousbias)\n",
    "    neurnet.losshistory = copy(neurnet.prevlosshistory)\n",
    "    return (neurnet.weights, neurnet.bias, neurnet.losshistory)\n",
    "end\n",
    "\n",
    "isfinitevec(a) = isfinite.(a)\n",
    "\n",
    "function hasdiverged(neurnet::DenseNeuralNetworkSoA)\n",
    "    return !(all(all.(isfinitevec.(neurnet.weights)) .&& all.(isfinitevec.(neurnet.bias))))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17549f15",
   "metadata": {},
   "source": [
    "To train the network, we use backpropagation with stochastic gradient descent. Essentially, because a change in the weight matrix affects only the layers in front of it, we can calculate the gradient at the last layer of the network, store how we would update the weight matrix, then take that information as given so we can calculate the gradient of the next to last layer. We iterate that process backwards through the network, then used our stored information to update all of our weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69717af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "function updateweights!(neurnet::DenseNeuralNetworkSoA, inputs, targets, numweightlayers,\n",
    "    δatlayer, learningrate;\n",
    "    preactivations=Vector{Vector{Float64}}(undef, numlayers(neurnet)),\n",
    "    postactivations=Vector{Vector{Float64}}(undef, numlayers(neurnet)))\n",
    "\n",
    "    for (input, target) in zip(inputs, targets)\n",
    "\n",
    "        (output, _, _) = forwardpass_keepvals!(neurnet, input,\n",
    "            preactivations, postactivations)\n",
    "\n",
    "        # Following is for SoA\n",
    "        outputerror = ((output .- target) .* neurnet.gradients[end](preactivations[end]))\n",
    "        δatlayer[end] = outputerror\n",
    "\n",
    "        for layerindex in (numweightlayers-1):-1:1\n",
    "            δatlayer[layerindex] = ((transpose(neurnet.weights[layerindex+1])\n",
    "                                     *\n",
    "                                     δatlayer[layerindex+1])\n",
    "                                    .*\n",
    "                                    neurnet.gradients[layerindex](preactivations[layerindex]))\n",
    "        end\n",
    "\n",
    "        for (layerindex, δ, postactivation) in zip(1:numweightlayers, δatlayer, postactivations)\n",
    "            # in zip(neurnet.weights, neurnet.bias, δatlayer, postactivations)\n",
    "            neurnet.weights[layerindex] .-= learningrate .* δ .* transpose(postactivation)\n",
    "            neurnet.bias[layerindex] .-= learningrate .* δ\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return neurnet\n",
    "end\n",
    "\n",
    "function train!(neurnet, inputs, targets; numepochs=10, learningrate=0.05)\n",
    "    if !equaldatalengths(inputs, targets)\n",
    "        error(\"Input and target arrays must be of the same length\")\n",
    "    end\n",
    "\n",
    "    updateprevweights!(neurnet)\n",
    "\n",
    "    numweightlayers = numlayers(neurnet) - 1\n",
    "    numnodeslayer = numnodesalllayers(neurnet)\n",
    "\n",
    "    lossatepoch = Vector{Float64}(undef, numepochs+1)\n",
    "    lossatepoch[begin] = predictionerror(neurnet, inputs, targets)\n",
    "\n",
    "    for epoch in 1:numepochs\n",
    "\n",
    "        δatlayer = Vector{Vector{Float64}}(undef, numweightlayers)\n",
    "\n",
    "        updateweights!(neurnet, inputs, targets, numweightlayers, δatlayer, learningrate)\n",
    "\n",
    "        if hasdiverged(neurnet)\n",
    "            forgetprevtraining!(neurnet)\n",
    "            error(\"Model has diverged. Try turning down the learning rate.\\n\\\n",
    "                Resetting to previous weights: $(prevweights(neurnet)) \\\n",
    "                | Epoch: $(epoch)\")\n",
    "        end\n",
    "\n",
    "        lossatepoch[epoch+1] = predictionerror(neurnet, inputs, targets)\n",
    "    end\n",
    "\n",
    "    neurnet.prevlosshistory = neurnet.losshistory\n",
    "    neurnet.losshistory = [neurnet.losshistory; lossatepoch]\n",
    "\n",
    "    return lossatepoch\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e01d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DenseNeuralNetworkSoA([784, 60, 60, 10])\n",
    "train!(net, trainfeatures, trainlabels; numepochs=4, learningrate=0.046)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8296384e",
   "metadata": {},
   "source": [
    "As you can see, the error goes down sharply at first, then plateaus as the algorithm circles a local minimum of the loss function. Our final classification error is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8679a7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([(onehotdecodedigits(predict(net, feature)) == label) \n",
    "        for (feature, label) in zip(testfeatures, testdata.targets)])/length(testfeatures)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
